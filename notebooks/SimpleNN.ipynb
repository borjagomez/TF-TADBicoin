{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "import pandas\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras.regularizers import L1L2\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"/data/coinapi.csv\")\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "x = dataset[30000:-100,1:-1].astype(float)\n",
    "y = dataset[30000:-100,-1:].astype(float)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_scaled = scaler.fit_transform(x)\n",
    "\n",
    "x_train = np.array(x_scaled[:-200,:])\n",
    "y_train = np.array(y[:-200,:])\n",
    "x_val = np.array(x_scaled[-200:,:])\n",
    "y_val = np.array(y[-200:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16034, 5) (16034, 1) (200, 5) (200, 1)\n",
      "[[0.96280625 0.96638015 0.96405172 0.96724624 0.02474742]\n",
      " [0.96582455 0.96606775 0.96355092 0.97137411 0.01661583]\n",
      " [0.96577237 0.9646703  0.96343687 0.97100901 0.02071144]\n",
      " [0.96488485 0.96369656 0.96232366 0.96708832 0.00839341]\n",
      " [0.96315347 0.95832417 0.96054663 0.96339874 0.0093457 ]\n",
      " [0.95845146 0.96064404 0.95798664 0.96462523 0.01027901]\n",
      " [0.96129858 0.96331493 0.9626366  0.9694239  0.0093384 ]\n",
      " [0.96300578 0.95915822 0.96205043 0.96628476 0.0057461 ]\n",
      " [0.95931462 0.95832608 0.95612809 0.96469597 0.00651288]\n",
      " [0.95824132 0.95667032 0.95647636 0.96369327 0.01055306]\n",
      " [0.9566672  0.95238958 0.95327948 0.95473336 0.01415148]\n",
      " [0.95238615 0.94924765 0.9509719  0.95592002 0.00622871]\n",
      " [0.94911875 0.95089211 0.94963025 0.95382341 0.00840579]\n",
      " [0.95088857 0.95214171 0.95033711 0.9576323  0.00906378]\n",
      " [0.95202468 0.95480391 0.95521744 0.95975948 0.00482087]\n",
      " [0.95417459 0.96236068 0.95929855 0.96258749 0.00336425]\n",
      " [0.96118221 0.95900376 0.95820319 0.96369763 0.00260707]\n",
      " [0.95888704 0.96194392 0.95858347 0.96543281 0.00641462]\n",
      " [0.96198762 0.95856525 0.95878883 0.96638276 0.00446388]\n",
      " [0.95900429 0.96179102 0.95928439 0.96690997 0.0017599 ]\n",
      " [0.96230387 0.96291434 0.96096032 0.96900307 0.00353002]\n",
      " [0.9637858  0.96228797 0.9605475  0.96698473 0.00707785]\n",
      " [0.96190621 0.9602551  0.9593575  0.96795495 0.00661108]\n",
      " [0.96011325 0.95588496 0.96535769 0.95834538 0.01137152]\n",
      " [0.95608253 0.96160577 0.95875577 0.95979791 0.00635486]\n",
      " [0.96230335 0.96054402 0.96358415 0.96922999 0.0085208 ]\n",
      " [0.96054118 0.96004481 0.95900469 0.96689617 0.0048882 ]\n",
      " [0.95977369 0.95934591 0.95844826 0.9656482  0.00376928]\n",
      " [0.95944109 0.96201367 0.95966904 0.96632633 0.00216112]\n",
      " [0.96236406 0.9570683  0.9590806  0.96544626 0.00302787]\n",
      " [0.95677331 0.95189959 0.95545551 0.95992578 0.00403779]\n",
      " [0.95200693 0.95062685 0.95363667 0.95641491 0.00851428]\n",
      " [0.95137721 0.94133152 0.94796009 0.94918319 0.01469626]\n",
      " [0.941571   0.94555467 0.94258053 0.94205156 0.01960486]\n",
      " [0.9456753  0.94841829 0.94707289 0.94877127 0.00919015]\n",
      " [0.94757559 0.9385867  0.94413751 0.9436393  0.00940437]\n",
      " [0.93796195 0.93128858 0.93883492 0.93724294 0.01152849]\n",
      " [0.93128363 0.9299113  0.92981418 0.92994973 0.01769338]\n",
      " [0.92960026 0.92107326 0.93074756 0.92710232 0.01571666]\n",
      " [0.92106253 0.91531471 0.92057531 0.92007044 0.01482937]\n",
      " [0.91485041 0.91962728 0.91901552 0.91914093 0.01390663]\n",
      " [0.91948493 0.92708473 0.92439998 0.92800091 0.00983825]\n",
      " [0.92640801 0.92932685 0.92718894 0.93260288 0.00635994]\n",
      " [0.92944736 0.93002645 0.92875712 0.93570516 0.00986568]\n",
      " [0.93047595 0.92891043 0.92814734 0.93424023 0.00357047]\n",
      " [0.92837857 0.92836199 0.92795948 0.9320196  0.01128069]\n",
      " [0.92826916 0.93105828 0.92752322 0.93329534 0.00885397]\n",
      " [0.93106271 0.92814404 0.92907093 0.93632583 0.00544144]\n",
      " [0.92800161 0.93238929 0.92901391 0.93653894 0.00532651]\n",
      " [0.93237816 0.93263698 0.93023854 0.93816005 0.00488479]\n",
      " [0.93277356 0.92502298 0.92990111 0.93145833 0.00587542]\n",
      " [0.92542168 0.93352444 0.93327029 0.92916101 0.00933416]\n",
      " [0.93355636 0.94025237 0.93734581 0.94213157 0.00521836]\n",
      " [0.9408216  0.93960861 0.93936878 0.94696291 0.00603523]\n",
      " [0.94019536 0.93674342 0.94261097 0.94218974 0.00846935]\n",
      " [0.9364628  0.93810748 0.93546329 0.94018206 0.59634421]\n",
      " [0.93818496 0.94051503 0.93889177 0.94677966 0.00214514]\n",
      " [0.94095015 0.93966967 0.93862274 0.94549693 0.0026712 ]\n",
      " [0.93947293 0.93680831 0.9370306  0.94147911 0.00315473]\n",
      " [0.93686742 0.9379932  0.93492453 0.94262349 0.00375425]\n",
      " [0.93811381 0.93926577 0.93747385 0.94545011 0.00424534]\n",
      " [0.93886495 0.93958582 0.93749379 0.94390919 0.00589405]\n",
      " [0.9400562  0.94103773 0.93938225 0.94804684 0.00555051]\n",
      " [0.94105975 0.93603391 0.94086174 0.94418695 0.0133801 ]\n",
      " [0.93615716 0.93354218 0.93265544 0.94033334 0.00436881]\n",
      " [0.93375693 0.93039764 0.93024186 0.93632583 0.0072251 ]\n",
      " [0.930127   0.92863995 0.92922259 0.93379076 0.01018125]\n",
      " [0.92767232 0.92481547 0.92770234 0.93323944 0.00700534]\n",
      " [0.92475143 0.91921816 0.92305465 0.92639169 0.01179804]\n",
      " [0.9182074  0.92268606 0.92257764 0.92547825 0.01057898]\n",
      " [0.92236111 0.924896   0.92308527 0.92829159 0.00747496]\n",
      " [0.92434351 0.92265353 0.92273525 0.92497113 0.0077739 ]\n",
      " [0.92206921 0.92943991 0.92589574 0.92837265 0.00915637]\n",
      " [0.92914137 0.93184711 0.92888884 0.93430085 0.00409057]\n",
      " [0.93198328 0.93289737 0.92964853 0.93816581 0.00792821]\n",
      " [0.93299361 0.93683909 0.9359592  0.94156646 0.0088997 ]\n",
      " [0.93702537 0.93979177 0.93630624 0.94231534 0.00869498]\n",
      " [0.93977335 0.95802359 0.95464037 0.94837473 0.03043843]\n",
      " [0.95806162 0.95528973 0.95623741 0.96101828 0.0183943 ]\n",
      " [0.95543385 0.95431687 0.95545324 0.96067519 0.02306895]\n",
      " [0.95431358 0.9538107  0.95273897 0.9564053  0.01609893]\n",
      " [0.95420868 0.95303978 0.95226248 0.95914196 0.01607047]\n",
      " [0.95306023 0.95522937 0.95308881 0.96010676 0.01188808]\n",
      " [0.95587779 0.95085402 0.95319482 0.95925533 0.00818435]\n",
      " [0.95112237 0.9489116  0.94941544 0.9541541  0.00947143]\n",
      " [0.9493609  0.95260997 0.95054002 0.95760872 0.00952478]\n",
      " [0.95255385 0.95691558 0.95352612 0.95521637 0.00733343]\n",
      " [0.95709269 0.96045357 0.95957493 0.96417331 0.01215322]\n",
      " [0.9605177  0.95138732 0.95962916 0.95839342 0.01136525]\n",
      " [0.9511422  0.95057953 0.94881371 0.95704238 0.00421012]\n",
      " [0.95057597 0.94851988 0.95061891 0.95658959 0.00293927]\n",
      " [0.94795186 0.95092307 0.94934635 0.95650225 0.00202524]\n",
      " [0.95108479 0.95216276 0.95197526 0.95946635 0.00485563]\n",
      " [0.95254672 0.95219407 0.95114473 0.95653719 0.00450673]\n",
      " [0.95136104 0.94712555 0.94949748 0.95204754 0.01087421]\n",
      " [0.9474747  0.94570409 0.95001053 0.94855396 0.02048381]\n",
      " [0.94531    0.9489149  0.94724553 0.95292115 0.01179642]\n",
      " [0.94883851 0.94407651 0.94705469 0.95227691 0.01280979]\n",
      " [0.94360645 0.94683124 0.94366627 0.95172332 0.01353325]\n",
      " [0.94624866 0.94550893 0.95185141 0.95375267 0.0160773 ]\n",
      " [0.945505   0.94445657 0.94580505 0.95156418 0.01708498]\n",
      " [0.94364611 0.93604261 0.94066688 0.93223813 0.05979185]\n",
      " [0.93564365 0.93433971 0.93463626 0.93938583 0.01498971]\n",
      " [0.9346307  0.93894589 0.93753402 0.94142321 0.01250782]\n",
      " [0.93851408 0.9419024  0.93870478 0.94603514 0.00933288]\n",
      " [0.94189821 0.94218557 0.93876618 0.9461431  0.01073399]\n",
      " [0.94146524 0.94183717 0.93919351 0.94596247 0.01402111]\n",
      " [0.94183298 0.94174602 0.93961787 0.94796229 0.01419655]\n",
      " [0.94171538 0.93565715 0.93824456 0.93941588 0.0092207 ]\n",
      " [0.93447223 0.93574499 0.93513514 0.94020791 0.00701386]\n",
      " [0.93595346 0.93766532 0.93722563 0.94358934 0.00721979]\n",
      " [0.93777564 0.93175666 0.93798602 0.93719926 0.01728109]\n",
      " [0.93162858 0.92896679 0.92876604 0.93214223 0.02677404]\n",
      " [0.92913111 0.93394295 0.93250938 0.93699733 0.01042832]\n",
      " [0.93393819 0.93577161 0.93618835 0.94017542 0.00913087]\n",
      " [0.93583326 0.93128371 0.93325647 0.93293654 0.01348039]\n",
      " [0.93119543 0.94260982 0.9396198  0.93809716 0.01262213]\n",
      " [0.94237363 0.93751016 0.94553217 0.94382028 0.01391577]\n",
      " [0.93842102 0.93494086 0.93551839 0.93632583 0.01495368]\n",
      " [0.93475978 0.9328445  0.93264075 0.93627342 0.00940013]\n",
      " [0.93324411 0.92804924 0.93081753 0.92846488 0.03399924]\n",
      " [0.92843354 0.91745334 0.92697081 0.92169154 0.03401103]\n",
      " [0.9171929  0.90808843 0.9198632  0.90683506 0.08283104]\n",
      " [0.90758238 0.91066504 0.9088305  0.90400861 0.04814906]\n",
      " [0.9104476  0.91607884 0.91349917 0.91717657 0.02830306]\n",
      " [0.9160728  0.91449196 0.91578611 0.92154201 0.02477351]\n",
      " [0.91494557 0.91010982 0.91454066 0.91580841 0.02100284]\n",
      " [0.91049074 0.91153232 0.91285738 0.91456446 0.03040857]\n",
      " [0.91152578 0.90277639 0.90873534 0.90959861 0.0259533 ]\n",
      " [0.90298631 0.89976126 0.90636007 0.90726269 0.04263504]\n",
      " [0.89963976 0.90477899 0.90109596 0.90124732 0.03248976]\n",
      " [0.90387261 0.90612114 0.90310249 0.9055808  0.01971633]\n",
      " [0.9067801  0.90603103 0.90588428 0.91192545 0.03157791]\n",
      " [0.90534723 0.91037786 0.90846613 0.91293742 0.01319422]\n",
      " [0.91037141 0.90139006 0.90858176 0.90875767 0.01406363]\n",
      " [0.90102722 0.90946014 0.90743287 0.9054131  0.01767798]\n",
      " [0.90945936 0.91123488 0.91082041 0.91495227 0.0215789 ]\n",
      " [0.91103418 0.91102284 0.90810125 0.91408407 0.01221742]\n",
      " [0.911197   0.91210738 0.91014521 0.91536331 0.01091104]\n",
      " [0.91192031 0.91812476 0.91753446 0.92007987 0.02162629]\n",
      " [0.91841371 0.91733245 0.91718916 0.92479644 0.02284071]\n",
      " [0.91726561 0.92191549 0.92086288 0.92577085 0.01226006]\n",
      " [0.92190065 0.92012667 0.92183335 0.92829212 0.02111873]\n",
      " [0.92048796 0.91838184 0.92090311 0.92444707 0.01663533]\n",
      " [0.91773181 0.91371478 0.91675973 0.92130269 0.00961241]\n",
      " [0.91382129 0.91534741 0.91374459 0.91823989 0.01137611]\n",
      " [0.91543682 0.91038587 0.91253308 0.91709568 0.0114842 ]\n",
      " [0.90957835 0.92775371 0.9249265  0.91711018 0.07408685]\n",
      " [0.92805032 0.92763073 0.92772508 0.93305934 0.07030787]\n",
      " [0.92750741 0.93211428 0.93229055 0.93494632 0.0349055 ]\n",
      " [0.93219794 0.93291146 0.9318389  0.9383122  0.01488698]\n",
      " [0.9335447  0.93311689 0.93297275 0.93952401 0.02143511]\n",
      " [0.9324173  0.93294173 0.93058279 0.93826381 0.01612073]\n",
      " [0.9328868  0.93569525 0.9363923  0.94055816 0.04254318]\n",
      " [0.93600704 0.93286885 0.93384736 0.93865756 0.01336039]\n",
      " [0.9326283  0.93178292 0.9303776  0.93861808 0.01253767]\n",
      " [0.93178966 0.93171317 0.92932299 0.93633491 0.00864318]\n",
      " [0.93170825 0.93578065 0.93629067 0.93816162 0.010084  ]\n",
      " [0.93590336 0.93902729 0.93553711 0.94197418 0.0056176 ]\n",
      " [0.93849373 0.94259121 0.94059289 0.94666349 0.00499749]\n",
      " [0.94270032 0.93686884 0.94084425 0.94504885 0.01674076]\n",
      " [0.9367046  0.93571229 0.93739391 0.94383058 0.01106966]\n",
      " [0.93616656 0.93753278 0.93740073 0.94301269 0.0085553 ]\n",
      " [0.93731762 0.9410005  0.93760766 0.9458484  0.00646699]\n",
      " [0.94091258 0.93675473 0.93938995 0.94531001 0.00749628]\n",
      " [0.93647272 0.93444894 0.93599191 0.94283993 0.01582098]\n",
      " [0.93455068 0.93821672 0.93523799 0.93994937 0.01928787]\n",
      " [0.93821227 0.93306888 0.93513426 0.94005261 0.01114482]\n",
      " [0.9328741  0.93573438 0.93227376 0.94144645 0.01232339]\n",
      " [0.93534636 0.94782097 0.94488496 0.94324084 0.02015972]\n",
      " [0.94831908 0.94639847 0.94941982 0.95346548 0.01716423]\n",
      " [0.9465877  0.945937   0.94608947 0.95189399 0.021444  ]\n",
      " [0.94593311 0.94477315 0.94742395 0.95131403 0.02013202]\n",
      " [0.9443188  0.9433209  0.94320255 0.94640198 0.03136917]\n",
      " [0.94324967 0.94108034 0.9403312  0.94663083 0.01929651]\n",
      " [0.94126832 0.94073489 0.93846531 0.94345308 0.0095964 ]\n",
      " [0.94067183 0.94428628 0.94155059 0.94711506 0.00895449]\n",
      " [0.94398689 0.94409738 0.94277504 0.95030084 0.00454203]\n",
      " [0.94409318 0.94383734 0.94235628 0.94916537 0.00621608]\n",
      " [0.94390409 0.94329359 0.9446147  0.95065668 0.00908852]\n",
      " [0.94322497 0.94570496 0.94303008 0.95132905 0.00238074]\n",
      " [0.94332725 0.93871368 0.93986539 0.94572402 0.00380268]\n",
      " [0.93846868 0.93925272 0.93687002 0.94315192 0.00504386]\n",
      " [0.93958687 0.93926386 0.93885381 0.94366271 0.00444179]\n",
      " [0.93929097 0.94431516 0.94105783 0.94700815 0.00516458]\n",
      " [0.94442248 0.94952301 0.94609192 0.95152994 0.0068061 ]\n",
      " [0.9495192  0.95816135 0.95477891 0.95816161 0.01693492]\n",
      " [0.95818235 0.98890767 0.98759818 0.96686123 0.10518789]\n",
      " [0.98918502 0.98553004 0.99021117 0.99074225 0.03826807]\n",
      " [0.98605    0.98164677 0.9854811  0.98906193 0.02331314]\n",
      " [0.98174391 0.97981098 0.98281896 0.9871484  0.02323662]\n",
      " [0.98001862 0.98057441 0.98262987 0.98787895 0.00990055]\n",
      " [0.98117768 0.98459701 0.98250602 0.98785868 0.01295997]\n",
      " [0.9852505  0.98338515 0.98561422 0.98917356 0.00796359]\n",
      " [0.98281651 0.98126775 0.98079528 0.98875675 0.00534304]\n",
      " [0.98127092 0.97273742 0.98206102 0.97723715 0.0199643 ]\n",
      " [0.97299934 0.98045961 0.97730296 0.97985886 0.01589009]\n",
      " [0.98027137 0.97876506 0.97729456 0.9851077  0.00725946]\n",
      " [0.9788145  0.97655529 0.97644619 0.98227514 0.01155763]\n",
      " [0.97622569 0.97567914 0.97783839 0.98200018 0.01483537]] [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)\n",
    "print(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(20, activation='sigmoid', input_dim=5))\n",
    "model.add(Dense(20, activation='sigmoid', input_dim=20))\n",
    "model.add(Dense(20, activation='sigmoid', input_dim=20))\n",
    "model.add(Dense(1, activation='softmax', input_dim=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "502/502 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 2/100\n",
      "502/502 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 3/100\n",
      "502/502 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 4/100\n",
      "502/502 [==============================] - 1s 1ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 5/100\n",
      "502/502 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 6/100\n",
      "502/502 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 7/100\n",
      "502/502 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 8/100\n",
      "502/502 [==============================] - 1s 2ms/step - loss: 0.0000e+00 - accuracy: 0.5122 - val_loss: 0.0000e+00 - val_accuracy: 0.4450\n",
      "Epoch 9/100\n",
      "392/502 [======================>.......] - ETA: 0s - loss: 0.0000e+00 - accuracy: 0.5148"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-b07e2832e234>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_build_call_outputs\u001b[0;34m(self, result)\u001b[0m\n\u001b[1;32m   2172\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2174\u001b[0;31m         \u001b[0moutputs_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2175\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m     ret = nest.pack_sequence_as(self._func_graph.structured_outputs,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
